"""
Individual run analysis using outputs from fMRIPrep

Adapted script from original notebook:
https://github.com/poldrack/fmri-analysis-vm/blob/master/analysis/postFMRIPREPmodelling/First%20and%20Second%20Level%20Modeling%20(FSL).ipynb

More information on what this script is doing - beyond the commented code - is provided on the lab's github wiki page
Nesting of functions: main > argparser > process_subject > create_firstlevel_workflow > data_grabber > get_mcparams > gen_model_info > read_contrasts > substitutes

Requirement: BIDS dataset (including events.tsv), derivatives directory with fMRIPrep outputs, and modeling files

"""
import nipype.algorithms.modelgen as model
from nipype.interfaces import fsl, ants
from nipype.interfaces.base import Bunch
from nipype import Workflow, Node, IdentityInterface, Function, DataSink, JoinNode, MapNode
import os
import os.path as op
import argparse
from bids.layout import BIDSLayout
from niflow.nipype1.workflows.fmri.fsl import create_susan_smooth
import pandas as pd
import glob

# use spatial smoothing? if True, kernel size is used 
run_smoothing=True
smoothing_kernel_size=6 # i.e., 6mm FWHM kernel

# define first level workflow function
def create_firstlevel_workflow(projDir, sub, task, derivDir, runs, outDir,
                               events, ses, TR, sparse, workDir, dropvols=0,
                               name='sub-{}_task-{}_events_levelone'):
    """Processing pipeline"""
    
    # initialize workflow
    wf = Workflow(name=name.format(sub, task),
                  base_dir=workDir)
    
    # configure workflow
    # parameterize_dirs: parameterizations over 32 characters will be replaced by their hash; essentially prevented an issue of having too many characters in a file/folder name (i.e., https://github.com/nipy/nipype/issues/2061#issuecomment-1189562017)
    wf.config['execution']['parameterize_dirs'] = False

    infosource = Node(IdentityInterface(fields=['run_id', 'event_file']), name='infosource')
    
    # define iterables to run nodes over runs, events, and/or subs
    infosource.iterables = [('run_id', runs),
                            ('event_file', events)]
    infosource.synchronize = True
    
    # define data grabber function
    def data_grabber(sub, task, derivDir, ses, run_id):
        """Quick filegrabber ala SelectFiles/DataGrabber"""
        import os.path as op
        
        # define output filename and path, depending on whether session information is in directory/file names
        if ses:
            # define path to preprocessed functional and mask data (subject derivatives func folder)
            prefix = 'sub-{}_ses-{}_task-{}_run-{:02d}'.format(sub, ses, task, run_id)
            funcDir = op.join(derivDir, 'sub-{}'.format(sub), 'ses-{}'.format(ses), 'func')
            mni_mask = op.join(funcDir, 'sub-{}_ses-{}_space-MNI152NLin2009cAsym_res-2_desc-brain_mask_allruns-BOLDmask.nii.gz'.format(sub, ses))
            
        else:
            # define path to preprocessed functional and mask data (subject derivatives func folder)
            prefix = 'sub-{}_task-{}_run-{:02d}'.format(sub, task, run_id)
            funcDir = op.join(derivDir, 'sub-{}'.format(sub), 'func')
            mni_mask = op.join(funcDir, 'sub-{}_space-MNI152NLin2009cAsym_res-2_desc-brain_mask_allruns-BOLDmask.nii.gz'.format(sub))

        # grab the confound and MNI file
        confound_file = op.join(funcDir, '{}_desc-confounds_timeseries.tsv'.format(prefix))
        mni_file = op.join(funcDir, '{}_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz'.format(prefix))
        
        # grab the outlier file generated by rapidart
        outlier_file = op.join(funcDir, 'art', '{}{:02d}'.format(task, run_id), 'art.{}_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold_outliers.txt'.format(prefix))
        
        return confound_file, outlier_file, mni_file, mni_mask
    
    datasource = Node(Function(output_names=['confound_file',
                                             'outlier_file',
                                             'mni_file',
                                             'mni_mask'],
                               function=data_grabber),
                      name='datasource')
    datasource.inputs.sub = sub
    datasource.inputs.task = task
    datasource.inputs.derivDir = derivDir
    datasource.inputs.ses = ses
    wf.connect(infosource, "run_id", datasource, "run_id")
      
    # if requested, smooth before running model
    if run_smoothing:
        # create_susan_smooth refers to FSL's Susan algorithm for smoothing data
        smooth = create_susan_smooth()
        
        # smoothing workflow requires the following inputs:
            # inputnode.in_files : functional runs (filename or list of filenames)
            # inputnode.fwhm : fwhm for smoothing with SUSAN
            # inputnode.mask_file : mask used for estimating SUSAN thresholds (but not for smoothing)
        
        # provide the mni file (read in from datastore Node above) as the inputnode.in_files
        wf.connect(datasource, "mni_file", smooth, 'inputnode.in_files')
        
        # provide the smoothing_kernel_size
        smooth.inputs.inputnode.fwhm = smoothing_kernel_size
        
        # provide mask file (read in from datastore Node above) as the inputnode.mask_file
        wf.connect(datasource, "mni_mask", smooth, 'inputnode.mask_file')

    # define model configuration function
    def gen_model_info(event_file, confound_file, regressor_names, dropvols):
        """Defines `SpecifyModel` information from BIDS events."""
        import pandas as pd
        from nipype.interfaces.base import Bunch
        
        # read in events file and identify trial types
        events = pd.read_csv(event_file, sep="\t")
        trial_types = events.trial_type.unique()
        onset = []
        duration = []
        
        # for each trial type
        for trial in trial_types:
            # extract onset and duration information
            onset.append(events[events.trial_type == trial].onset.tolist())
            duration.append(events[events.trial_type == trial].duration.tolist())
        
        # read in confounds file
        confounds = pd.read_csv(confound_file, sep='\t', na_values='n/a')
        regressors = []
        
        # for each regressor
        # NOTE: we are not using framewise displacement or the realigment motion parameters as regressors as is often done with adult fMRI
        for regressor in regressor_names:
            regressors.append(confounds[regressor][dropvols:])

        info = [Bunch(
            conditions=trial_types,
            onsets=onset,
            durations=duration,
            regressors=regressors,
            regressor_names=regressor_names,
        )]
        return info

    modelinfo = Node(Function(function=gen_model_info), name='modelinfo')
    modelinfo.inputs.dropvols = dropvols

    # could format as bids model json in the future (https://bids-standard.github.io/stats-models/walkthrough-1.html)
    # need to add artifact volumes output from art in prior step [see below for how outlier_files were passed to modelspec]
    modelinfo.inputs.regressor_names = [
        'a_comp_cor_00',
        'a_comp_cor_01',
        'a_comp_cor_02',
        'a_comp_cor_03',
        'a_comp_cor_04',
    ]
    
    # from infosource node add event and confound files as input to modelinfo node
    wf.connect(infosource, 'event_file', modelinfo, 'event_file')
    wf.connect(datasource, 'confound_file', modelinfo, 'confound_file')
    wf.connect(datasource, 'outlier_file', modelinfo, 'outlier_files') # generated using rapidart in motion exclusions script
    
    # if drop values requested (likely always no for us)
    if dropvols:
        roi = Node(fsl.ExtractROI(t_min=dropvols, t_size=-1), name="extractroi")
        # pass these data to the smooth node, if smoothing was requested
        if run_smoothing:
            wf.connect(smooth, "outputnode.smoothed_files", roi, "in_file")
        # pass these data to the workflow skipping smoothing if not requested
        else: 
            wf.connect(datasource, "mni_file", roi, "in_file")
    
    # if sparse model requested (allows model generation for sparse and sparse-clustered acquisition experiments)
    if sparse:
        modelspec = Node(model.SpecifySparseModel(), name="modelspec")
        modelspec.inputs.time_acquisition = None
    else:
        modelspec = Node(model.SpecifyModel(), name="modelspec")
        
    # specify model inputs
    modelspec.inputs.input_units = "secs"
    modelspec.inputs.time_repetition = TR
    modelspec.inputs.high_pass_filter_cutoff = 210. # NOTE - THIS MAY CHANGE FOR DIFFERENT STUDIES
    wf.connect(modelinfo, 'out', modelspec, 'subject_info')
    
    # if drop values requested (likely always no for us)
    if dropvols:
        # pass dropped value files (smoothed or not depending on logic above) as functional runs to modelspec
        wf.connect(roi, "roi_file", modelspec, "functional_runs")
    else:
        if run_smoothing:
            # pass smoothed output files as functional runs to modelspec
            wf.connect(smooth, "outputnode.smoothed_files", modelspec, "functional_runs")
        else: 
           # pass unsmoothed output files as functional runs to modelspec
            wf.connect(datasource, "mni_file", modelspec, "functional_runs")

    # define function to read in and parse task contrasts
    def read_contrasts(projDir, task):
        import os.path as op
        import pandas as pd 

        contrasts = []
        contrasts_file = op.join(projDir, 'data', 'contrast_files', 'contrasts.tsv')
        
        # if contrasts file not found
        if not op.exists(contrasts_file):
            raise FileNotFoundError("Contrasts file not found.")
        
        # read in contrasts file
        info = pd.read_csv(contrasts_file, sep='\t')
        
        # for each row
        for index, row in info.iterrows():
            # skip a row specifies a contrast for a different task (i.e., not pixar)
            if row[0] != task:
                continue
            
            # extract task contrasts
            contrasts.append([
                row[1],
                "T",
                [cond for cond in row[2].split(" ")],
                [float(w) for w in row[3].split(" ")]
            ])
        
        # raise if there are no contrasts in the file for the current task
        if not contrasts:
            raise AttributeError("No contrasts found for task {}".format(task))
        return contrasts

    contrastgen = Node(Function(output_names=["contrasts"],
                                function=read_contrasts),
                       name="contrastgen")
    contrastgen.inputs.projDir = projDir
    contrastgen.inputs.task = task

    # provide first-level design parameters
    level1design = Node(fsl.Level1Design(), name="level1design")
    level1design.inputs.interscan_interval = TR
    level1design.inputs.bases = {"dgamma": {"derivs": False}}
    level1design.inputs.model_serial_correlations = True
    wf.connect(modelspec, "session_info", level1design, "session_info")
    wf.connect(contrastgen, "contrasts", level1design, "contrasts")

    # use FSL FEAT for GLM
    modelgen = Node(fsl.FEATModel(), name="modelgen")
    wf.connect(level1design, "fsf_files", modelgen, "fsf_file")
    wf.connect(level1design, "ev_files", modelgen, "ev_files")

    # MapNode is a special version of Node that will create an instance of the Node [here ApplyMask()] for every item in the list from the input
    # interfield defines which input should be iterated over
    # after running, the outputs are collected into a list to pass to the next Node
    masker = MapNode(fsl.ApplyMask(), name="masker", iterfield=['in_file'])
    wf.connect(datasource, "mni_mask", masker, "mask_file")

    # if drop vols was requested
    if dropvols:
        wf.connect(roi, "roi_file", masker, "in_file")
    else:
        # if smoothing was requested
        if run_smoothing: 
            wf.connect(smooth, "outputnode.smoothed_files", masker, "in_file")
        else:
            # if smoothing was not requested
            wf.connect(datasource, "mni_file", masker, "in_file")

    # configure GLM over design matrics
    glm = MapNode(fsl.FILMGLS(), name="filmgls", iterfield=['in_file'])
    if run_smoothing: 
        glm.inputs.mask_size = smoothing_kernel_size
        glm.inputs.smooth_autocorr = True
    wf.connect(masker, "out_file", glm, "in_file")
    wf.connect(modelgen, "design_file", glm, "design_file")
    wf.connect(modelgen, "con_file", glm, "tcon_file")
    wf.connect(modelgen, "fcon_file", glm, "fcon_file")

    # rename contrast output files with better filenames
    def substitutes(contrasts):
        """Datasink output path substitutes"""
        subs = []
        for i, con in enumerate(contrasts,1):
            # replace annoying chars in filename
            name = con[0].replace(" ", "").replace(">", "_gt_").lower()

            subs.append(('/cope%d.' % i, '/con_%d_%s_cope.' % (i,name)))
            subs.append(('/varcope%d.' % i, '/con_%d_%s_varcope.' % (i,name)))
            subs.append(('/zstat%d.' % i, '/con_%d_%s_zstat.' % (i, name)))
            subs.append(('/tstat%d.' % i, '/con_%d_%s_tstat.' % (i, name)))
            subs.append(('/_filmgls0/', '/'))
        return subs

    gensubs = Node(Function(function=substitutes), name="substitute_gen")
    wf.connect(contrastgen, "contrasts", gensubs, "contrasts")

    # stats should equal number of conditions...
    sinker = Node(DataSink(), name="datasink")
    sinker.inputs.base_directory = outDir
    sinker.inputs.regexp_substitutions = [("_event_file.*run_id_", "run"),
                                          ("model/sub.*_run-", "model/run"),
                                          ("_bold_space-MNI","/MNI"),
                                          ("_space-MNI","/MNI")]

    # define where output files are returned and where they are saved
    wf.connect(gensubs, "out", sinker, "substitutions")
    wf.connect(modelgen, "design_file", sinker, "design.@design_file")
    wf.connect(modelgen, "con_file", sinker, "design.@tcon_file")
    wf.connect(modelgen, "design_cov", sinker, "design.@cov")
    wf.connect(modelgen, "design_image", sinker, "design.@design")
    wf.connect(glm, "copes", sinker, "model.@copes")
    wf.connect(glm, "dof_file", sinker, "model.@dof")
    wf.connect(glm, "logfile", sinker, "model.@log")
    wf.connect(glm, "param_estimates", sinker, "model.@pes")
    wf.connect(glm, "residual4d", sinker, "model.@res")
    wf.connect(glm, "sigmasquareds", sinker, "model.@ss")
    wf.connect(glm, "thresholdac", sinker, "model.@thresh")
    wf.connect(glm, "tstats", sinker, "model.@tstats")
    wf.connect(glm, "varcopes", sinker, "model.@varcopes")
    wf.connect(glm, "zstats", sinker, "model.@zstats")
    #wf.connect(datasource, "mni_mask", sinker, "model.@mask_file")
    return wf

# define function to extract subject-level data for workflow
def process_subject(layout, projDir, sub, task, derivDir, ses, outDir, workDir, sparse):
    """Grab information and start nipype workflow
    We want to parallelize runs for greater efficiency
    """
    
    # identify and read in scans file (from derivDir bc artifact information is saved in that processed scans.tsv file)
    scans_tsv = glob.glob(op.join(derivDir, 'sub-{}'.format(sub), 'ses-{}'.format(ses), 'func', '*_scans.tsv'))[0]
    scans_df = pd.read_csv(scans_tsv, sep='\t')

    # extract subject, task, and run information from filenames in scans.tsv file
    scans_df['task'] = scans_df['filename'].str.split('task-', expand=True).loc[:,1]
    scans_df['task'] = scans_df['task'].str.split('_run', expand=True).loc[:,0]
    scans_df['task'] = scans_df['task'].str.split('_bold', expand=True).loc[:,0]
    scans_df['run'] = scans_df['filename'].str.split('run-', expand=True).loc[:,1]
    scans_df['run'] = scans_df['run'].str.split('_bold', expand=True).loc[:,0]
    
    # remove runs tagged with excessive motion in scans files or that are for a different task
    keepruns = scans_df[(scans_df.MotionExclusion == False) & (scans_df.task == task)].run
    keepruns = list(keepruns.astype(int).values)

    # if the participant didn't have any runs for this task or all runs were excluded due to motion
    if not keepruns:
        raise FileNotFoundError(
            'No included bold {} runs found for sub-{}'.format(task, sub)
        )

    # identify events files
    if ses: # if session was provided
        events_all = glob.glob(op.join(derivDir, 'sub-{}'.format(sub), 'ses-{}'.format(ses), 'func', 'sub-{}_ses-{}_task-{}_*_events.tsv'.format(sub, ses, task)))
       
    else: # if session was not provided
        events_all = glob.glob(op.join(derivDir, 'sub-{}'.format(sub), 'func', 'sub-{}_task-{}_*_events.tsv'.format(sub, task)))
    
    # extract TR info from bidsDir bold json files (assumes TR is same across runs)
    epi = layout.get(subject=sub, suffix='bold', task=task, return_type='file')[0] # take first file
    TR = layout.get_metadata(epi)['RepetitionTime'] # extract TR field
    
    # extract events in each run of data
    events = [] # initialize output
    for run in keepruns: # for each retained run
        ev_match = [evfile for evfile in events_all if 'run-{:02d}'.format(run) in evfile]
        events.append(ev_match[0])
        
    # if no events identified (e.g., resting state data which is not currently collected)
    if not events:
        raise FileNotFoundError(
            'No event files found for sub-{}'.format(sub)
        )

    # define subject output directory
    suboutdir = op.join(outDir, 'sub-{}'.format(sub)) # could include task folder but output will be saved in pixar specific folder

    # call firstlevel workflow with extracted subject-level data
    wf = create_firstlevel_workflow(projDir, sub, task, derivDir, keepruns,
                                    suboutdir, events, ses, TR, sparse, workDir)
                                   
    return wf

# define command line parser function
def argparser():
    # create an instance of ArgumentParser
    parser = argparse.ArgumentParser()
    
    # attach argument specifications to the parser
    parser.add_argument("projDir",
                        help="Project directory")
    parser.add_argument("-b", dest="bidsDir",
                        help="Root BIDS directory")
    parser.add_argument("-d", dest="derivDir",
                        help="Output directory of fMRIPrep")
    parser.add_argument("-w", dest="workDir", default=os.getcwd(),
                        help="Working directory")
    parser.add_argument("-o", dest="outDir", default=os.getcwd(),
                        help="Output directory")
    parser.add_argument("-ss", dest="ses",
                        help="Session to process (default: None)")                                            
    parser.add_argument("-s", dest="subjects", nargs="*",
                        help="List of subjects to process (default: all)")
    parser.add_argument("-sparse", action="store_true",
                        help="Specify a sparse model")
    parser.add_argument("-p", dest="plugin",
                        help="Nipype plugin to use (default: MultiProc)")
    return parser

# define main function that runs functions defined above
def main(argv=None):
    # call argparser function that defines command line inputs
    parser = argparser()
    args = parser.parse_args(argv)
    
    # print if BIDS directory is not found
    if not op.exists(args.bidsDir):
        raise IOError("BIDS directory {} not found.".format(args.bidsDir))
    
    # print if the fMRIPrep directory is not found
    if not op.exists(args.derivDir):
        raise IOError("fMRIprep directory {} not found.".format(args.derivDir))

    # print if the project directory is not found
    if not op.exists(args.projDir):
        raise IOError("Project directory {} not found.".format(args.projDir))

    # make working and output directories
    workDir, outDir = op.realpath(args.workDir), op.realpath(args.outDir)
    os.makedirs(workDir, exist_ok = True)
    
    # get layout of BIDS directory
    # this is necessary because the pipeline reads the functional json files that have TR info
    # the derivDir (where fMRIPrep outputs are) doesn't have json files with this information, so getting the layout of that directory will result in an error
    layout = BIDSLayout(args.bidsDir)
    #layout = BIDSLayout('/EBC/preprocessedData/TEBC-5y/BIDs_data/pilot')

    # define task
    # this could be done in the script call, but the wrapper script is meant to be general for all tasks
    # this pipeline script is specific to the pixar data
    task='pixar'

    # define subjects - if none are provided in the script call, they are extracted from the BIDS directory layout information
    subjects = args.subjects if args.subjects else layout.get_subjects()

    # for each subject in the list of subjects
    for sub in subjects:
        # create a process_subject workflow with the inputs defined above
        wf = process_subject(layout, args.projDir, sub, task, args.derivDir,
                             args.ses, outDir, workDir, args.sparse)
   
        # configure workflow options
        wf.config['execution'] = {'crashfile_format': 'txt',
                                  'remove_unnecessary_outputs': False,
                                  'keep_inputs': True}

        # run multiproc unless plugin specified in script call
        plugin = args.plugin if args.plugin else "MultiProc"
        args_dict = {'n_procs' : 4}
        wf.run(plugin=plugin, plugin_args = args_dict)

# execute code when file is run as script (the conditional statement is TRUE when script is run in python)
if __name__ == "__main__":
    main()